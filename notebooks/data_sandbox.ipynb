{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['_links' '_embedded' 'yoast_head' 'yoast_head_json'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m resp \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mhttps://www.metro.net/wp-json/wp/v2/line-override/\u001b[39m\u001b[39m'\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,headers\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m     12\u001b[0m df\u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_json(resp\u001b[39m.\u001b[39mtext)\n\u001b[1;32m---> 14\u001b[0m cleaned_df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mdrop([\u001b[39m'\u001b[39;49m\u001b[39m_links\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m_embedded\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39myoast_head\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39myoast_head_json\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\hikou\\.conda\\envs\\metro\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hikou\\.conda\\envs\\metro\\lib\\site-packages\\pandas\\core\\frame.py:4901\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   4770\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   4771\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4778\u001b[0m     errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   4779\u001b[0m ):\n\u001b[0;32m   4780\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4781\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4782\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4899\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4900\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   4902\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   4903\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   4904\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   4905\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   4906\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   4907\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   4908\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   4909\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hikou\\.conda\\envs\\metro\\lib\\site-packages\\pandas\\core\\generic.py:4147\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4145\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4146\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4147\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4149\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\hikou\\.conda\\envs\\metro\\lib\\site-packages\\pandas\\core\\generic.py:4182\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4180\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4181\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4182\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4183\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreindex(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{axis_name: new_axis})\n\u001b[0;32m   4185\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4186\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hikou\\.conda\\envs\\metro\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6018\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6016\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6017\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6018\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabels[mask]\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6019\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6020\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['_links' '_embedded' 'yoast_head' 'yoast_head_json'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import urllib.request, json \n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "\n",
    "\n",
    "# resp = requests.get('https://www.metro.net/wp-json/wp/v2/line-override/')\n",
    "# data = resp.json()\n",
    "# pd.DataFrame(data)\n",
    "\n",
    "resp = requests.get('https://www.metro.net/wp-json/wp/v2/line-override/', timeout=10,headers={'Content-Type': 'application/json'})\n",
    "df= pd.read_json(resp.text)\n",
    "\n",
    "cleaned_df = df.drop(['_links','_embedded','yoast_head','yoast_head_json'])\n",
    "\n",
    "# cleaned_df.to_csv('resp.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# to load the sample data we need pandas\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mtest_data.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m sample_route_data \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mto_dict(\u001b[39m'\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# sample_route_data_json = df.to_json(orient='index',index=['direction_id','stop_sequence'],indent=4)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hikou\\.conda\\envs\\metro\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hikou\\.conda\\envs\\metro\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    571\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    572\u001b[0m     dialect,\n\u001b[0;32m    573\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    583\u001b[0m )\n\u001b[0;32m    584\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 586\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\hikou\\.conda\\envs\\metro\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    479\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    481\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    484\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\hikou\\.conda\\envs\\metro\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwds:\n\u001b[0;32m    809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 811\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\hikou\\.conda\\envs\\metro\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1036\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1037\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown engine: \u001b[39m\u001b[39m{\u001b[39;00mengine\u001b[39m}\u001b[39;00m\u001b[39m (valid options are \u001b[39m\u001b[39m{\u001b[39;00mmapping\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1039\u001b[0m \u001b[39m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[39mreturn\u001b[39;00m mapping[engine](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\hikou\\.conda\\envs\\metro\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     48\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39musecols\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musecols\n\u001b[0;32m     50\u001b[0m \u001b[39m# open handles\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open_handles(src, kwds)\n\u001b[0;32m     52\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hikou\\.conda\\envs\\metro\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_handles\u001b[39m(\u001b[39mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m    223\u001b[0m         src,\n\u001b[0;32m    224\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    225\u001b[0m         encoding\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    226\u001b[0m         compression\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    227\u001b[0m         memory_map\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    228\u001b[0m         storage_options\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    229\u001b[0m         errors\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    230\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hikou\\.conda\\envs\\metro\\lib\\site-packages\\pandas\\io\\common.py:701\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    697\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    700\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    702\u001b[0m             handle,\n\u001b[0;32m    703\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    704\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    705\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    706\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    707\u001b[0m         )\n\u001b[0;32m    708\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    710\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_data.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "# to load the sample data we need pandas\n",
    "df = pd.read_csv('test_data.csv')\n",
    "sample_route_data = df.to_dict('records')\n",
    "# sample_route_data_json = df.to_json(orient='index',index=['direction_id','stop_sequence'],indent=4)\n",
    "\n",
    "sample_route_data_json = df.groupby('direction_id').apply(lambda x: x.to_json(orient='records'))\n",
    "\n",
    "\n",
    "# d = {'departure_times':'departure_time'}\n",
    "\n",
    "# works 2/6/2023\n",
    "# j = df.groupby(['day_type']).apply(lambda x: x[['departure_times']].rename(columns=d).to_dict('records')).reset_index(name='depature_times').to_json('test_json3.json',orient='records',indent=4)\n",
    "data_by_direction_id = df.groupby('direction_id').apply(lambda x: x.to_json(orient='records'))\n",
    "group_direction_id_by_stop_id = df.groupby(['direction_id','stop_id']).apply(lambda x: x.to_json(orient='records'))\n",
    "\n",
    "group_direction_id_by_stop_id_by_day_type = df.groupby(['direction_id','stop_id','day_type']).apply(lambda x: x.rename(columns=d).to_dict('records'))\n",
    "group_direction_id_by_day_type_by_stop_sequence = df.groupby(['direction_id','day_type','stop_sequence']).apply(lambda x: x.rename(columns=d).to_dict('records'))\n",
    "\n",
    "D = (group_direction_id_by_stop_id_by_day_type\n",
    "  .groupby(level=0)\n",
    "  .apply(lambda df: df.xs(df.name).to_dict())\n",
    "  .to_dict()\n",
    ")\n",
    "\n",
    "\n",
    "# PANDAS IS VERY HELPFUL WHEN YOU ARE DOING:\n",
    "#  - A JOIN (merging data from different tables)\n",
    "#  - A GROUP BY (aggregating data by a certain column)\n",
    "#  - TO ADD DATA TO A DATABASE QUICKLY\n",
    "\n",
    "\n",
    "# 1. group data by direction_id\n",
    "# 2. for each direction_id, group data by stop_id\n",
    "# 3. for each stop_id, group data by day type <-- store this data as an object called \"departure times\"\n",
    "\n",
    "\n",
    "# d = {'departure_times':'departure_time'}\n",
    "# df.rename(columns=d, inplace=True)\n",
    "# works as of 2.6.2023\n",
    "d = (df.groupby(['direction_id','stop_id','day_type'])['departure_time','day_type']\n",
    "       .apply(lambda x: x.to_dict('r'))\n",
    "       .reset_index(name='data')\n",
    "       .groupby(['direction_id','stop_id'])['day_type','data']\n",
    "       .apply(lambda x: x.set_index('day_type')['data'].to_dict())\n",
    "       .reset_index(name='departure_times')\n",
    "       )\n",
    "# group_direction_id_by_stop_id_by_day_type\n",
    "group_direction_id_by_day_type_by_stop_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_direction_id_by_stop_id_by_day_type.head(5)\n",
    "df = pd.read_csv('test_data.csv')\n",
    "def clean_up_group(x):\n",
    "    #  x.set_index('day_type')['data'].to_dict()\n",
    "     all_data = x.to_dict('records')\n",
    "#      print(all_data)\n",
    "     return all_data\n",
    "\n",
    "\n",
    "# the function works!! stopped here 2/9/2023\n",
    "# we need to get this to run on the entire dataframe in the larger script\n",
    "def get_departure_times(target_data):\n",
    "       initial_json = (target_data.groupby(['direction_id','day_type'])['departure_times','stop_sequence','stop_name','stop_id','coordinates']\n",
    "       .apply(lambda x: x.to_dict('records'))\n",
    "       .reset_index()\n",
    "       .rename(columns={0:'data'})\n",
    "       .groupby('direction_id')['day_type','data']\n",
    "       .apply(lambda x:x.set_index('day_type')['data'].to_dict())\n",
    "       .to_json(orient='index',indent=4)\n",
    "       )\n",
    "       cleaned_json = [json.loads(initial_json)]\n",
    "       # initial_json.to_dict()\n",
    "       return cleaned_json\n",
    "\n",
    "final_result = pd.DataFrame()\n",
    "final_result['route_code'] = df['route_code'].unique().astype('str')\n",
    "final_result['route_id'] = df['route_id'].unique().astype('str')\n",
    "final_result['payload'] = get_departure_times(df)\n",
    "\n",
    "print(final_result.to_json('final_result.json',orient='records',indent=4))\n",
    "\n",
    "test_json_10 = (df.groupby(['direction_id','day_type'])['departure_times','stop_sequence','stop_name','stop_id','coordinates']\n",
    "       .apply(lambda x: x.to_dict('r'))\n",
    "       .reset_index(name='data')\n",
    "       .groupby('direction_id')['day_type','data']\n",
    "\n",
    "       .apply(lambda x: x.set_index('day_type')['data'].to_dict())\n",
    "       .to_json('test_json10.json',orient='index',indent=4)\n",
    "       )\n",
    "get_departure_times(df)\n",
    "# d = (df.groupby(['direction_id','day_type','stop_sequence'])['departure_time','day_type']\n",
    "#        .apply(lambda x: x.to_dict('r'))\n",
    "#        .reset_index(name='data')\n",
    "#        .groupby(['direction_id','day_type'])['stop_sequence','data']\n",
    "#        .apply(lambda x: x.set_index('stop_sequence')['data'].to_dict())\n",
    "#        .reset_index(name='departure_times')\n",
    "#        .to_json('test_json10.json',orient='records',indent=4)\n",
    "#        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sqlalchemy import create_engine,MetaData,event\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import secrets as Config\n",
    "\n",
    "# from .utils.log_helper import *\n",
    "\n",
    "from secrets import *\n",
    "engine = create_engine(Config.API_DB_URI, echo=False)\n",
    "\n",
    "Session = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "session = Session()\n",
    "target_schema = \"metro_api_dev\"\n",
    "Base = declarative_base(metadata=MetaData(schema=target_schema))\n",
    "\n",
    "def get_db():\n",
    "    db = Session()\n",
    "    try:\n",
    "        print('Connected to the database')\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "# df = pd.read_sql_query()\n",
    "# print('df.head(5)')\n",
    "# print(df.head(5))\n",
    "all_route_data= pd.read_csv('final_df.csv')\n",
    "all_route_data.drop(columns=['id'],inplace=True)\n",
    "all_route_data.sort_values(by=['route_code'],inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "def get_individual_route_data(target_data):\n",
    "       target_data['day_type'] = target_data['day_type'].astype('str')\n",
    "       initial_json = (target_data.groupby(['direction_id','day_type'])\n",
    "       .apply(lambda x: x.to_dict('records'))\n",
    "       .reset_index()\n",
    "       .rename(columns={0:'data'})\n",
    "       .groupby('direction_id')['day_type','data']\n",
    "       .apply(lambda x:x.set_index('day_type')['data'].to_dict())\n",
    "       .to_json(orient='index',indent=4)\n",
    "       )\n",
    "       cleaned_json = [json.loads(initial_json)]       \n",
    "    #    cleaned_json = [json.loads(initial_json)]       \n",
    "       # initial_json.to_dict()\n",
    "       return cleaned_json\n",
    "\n",
    "def get_departure_times(route):\n",
    "    target_data = all_route_data.loc[all_route_data['route_code'].astype('str') == route]\n",
    "    if target_data.shape[0] > 0:\n",
    "        payload = []\n",
    "        payload.append(get_individual_route_data(target_data))\n",
    "        final_payload = pd.json_normalize(payload).assign(route_code=route).to_dict('records')\n",
    "        return final_payload\n",
    "    del target_data\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "route_code_df = pd.DataFrame({'route_code':all_route_data['route_code'].unique().astype('str')})\n",
    "route_id_df = pd.DataFrame({'route_id':all_route_data['route_id'].unique().astype('str')})\n",
    "final_df = pd.concat([route_code_df,route_id_df],ignore_index=True, axis=1)\n",
    "final_df.columns = ['route_code','route_id']\n",
    "# final_df['payload'] = final_df['route_code'].apply(lambda x: get_departure_times(x))\n",
    "final_df['payload'] = final_df['route_code'].map(get_departure_times)\n",
    "\n",
    "final_df['payload'] = final_df['payload'].astype('str')\n",
    "final_df['payload'] = final_df['payload'].str.replace('0.weekday','weekday')\n",
    "final_df['payload'] = final_df['payload'].str.replace('0.saturday','saturday')\n",
    "final_df['payload'] = final_df['payload'].str.replace('0.sunday','sunday')\n",
    "final_df['payload'] = final_df['payload'].str.replace('0.nan','none')\n",
    "final_df['payload'] = final_df['payload'].str.replace('1.weekday','weekday')\n",
    "final_df['payload'] = final_df['payload'].str.replace('1.saturday','saturday')\n",
    "final_df['payload'] = final_df['payload'].str.replace('1.sunday','sunday')\n",
    "final_df['payload'] = final_df['payload'].str.replace('1.nan','none')\n",
    "final_df['payload'] = final_df['payload'].str.replace('1.weekday','weekday')\n",
    "final_df['payload'] = final_df['payload'].str.replace('1.saturday','saturday')\n",
    "final_df['payload'] = final_df['payload'].str.replace('1.sunday','sunday')\n",
    "final_df['payload'] = final_df['payload'].str.replace('1.nan','none')\n",
    "final_df['payload'] = final_df['payload'].str.replace('\"','')\n",
    "final_df.head().to_csv('final_evil_df.csv',index=False)\n",
    "final_df.head().to_json('final_evil_df.json',orient='index',indent=4)\n",
    "# final_df['payload'] = final_df['payload']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df\n",
    "final_df.head().to_csv('final_evil_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(group_direction_id_by_stop_id_by_day_type.to_frame())\n",
    "\n",
    "# resource from stackoverflow: https://stackoverflow.com/questions/52923685/convert-pandas-multiindex-series-to-json-python\n",
    "# method a\n",
    "final_result = (df.groupby(['direction_id','stop_id'])['departure_time','day_type']\n",
    "       .apply(lambda x: x.to_dict('r'))\n",
    "       .reset_index(name='data')\n",
    "       .groupby('direction_id')['stop_id','data']\n",
    "       .apply(lambda x: x.set_index('stop_id')['data'].to_dict())\n",
    "       .to_json('test_json8.json',orient='index',indent=4)\n",
    "       )\n",
    "\n",
    "# method b\n",
    "\n",
    "d = (df.groupby(['direction_id','stop_id','day_type'])['departure_time','day_type']\n",
    "       .apply(lambda x: x.to_dict('r'))\n",
    "       .reset_index(name='data')\n",
    "       .groupby(['direction_id','stop_id'])['day_type','data']\n",
    "       .apply(lambda x: x.set_index('day_type')['data'].to_dict())\n",
    "       .reset_index(name='departure_times')\n",
    "       # .apply(lambda x: x.set_index('day_type')['data'].to_dict())\n",
    "       # .groupby('stop_id')['direction_id','data']\n",
    "       # .apply(lambda x: x.set_index('direction_id')['data'].to_dict())\n",
    "       .to_json('test_json6.json',orient='index',indent=4)\n",
    "       )\n",
    "\n",
    "\n",
    "print(d)\n",
    "# final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new code\n",
    "d = {'departure_times':'departure_time'}\n",
    "\n",
    "def process_data(x):\n",
    "    # x.drop('day_type', axis=1, inplace=True)  \n",
    "    # print(x.day_type)\n",
    "    result = x.set_index('day_type')['data'].to_dict()\n",
    "    # result.to_json('test_reulst.json',orient='index',indent=4)\n",
    "    return result\n",
    "\n",
    "df.rename(columns=d, inplace=True)\n",
    "d = (df.groupby(['direction_id','stop_id','day_type'])['departure_time','day_type']\n",
    "       .apply(lambda x: x.to_dict('r'))\n",
    "       .reset_index(name='data')\n",
    "       .groupby(['direction_id','stop_id'])['day_type','data']\n",
    "       .apply(lambda x: process_data(x))\n",
    "       .reset_index(name='departure_times')\n",
    "       # .apply(lambda x: x.set_index('day_type')['data'].to_dict())\n",
    "       # .groupby('stop_id')['direction_id','data']\n",
    "       # .apply(lambda x: x.set_index('direction_id')['data'].to_dict())\n",
    "    #    .to_json('test_json8.json',orient='index',indent=4)\n",
    "       )\n",
    "\n",
    "e = d.groupby(['direction_id'])['stop_id','departure_times'].apply(lambda x: x.set_index('stop_id')['departure_times'].to_dict()).reset_index(name='stop_id')\n",
    "e.to_json('test_json6.json',orient='records',indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'departure_times':'departure_time'}\n",
    "df.rename(columns=d, inplace=True)\n",
    "# dataframe_by_direction_id = df.groupby('direction_id').apply(lambda x: x.to_dict('r')).to_frame()\n",
    "# dataframe_by_direction_id = dataframe_by_direction_id.rename(columns={0:'route_id'}).apply(lambda x: x.to_dict('r')).to_frame()\n",
    "# print(dataframe_by_direction_id)\n",
    "# from collections import defaultdict\n",
    "\n",
    "d = (df.groupby(['direction_id','stop_id','day_type'])['departure_time','day_type']\n",
    "       .apply(lambda x: x.to_dict('r'))\n",
    "       .reset_index(name='data')\n",
    "       .groupby(['direction_id','stop_id'])['day_type','data']\n",
    "       .apply(lambda x: x.set_index('day_type')['data'].to_dict())\n",
    "       .reset_index(name='departure_times')\n",
    "       # .apply(lambda x: x.set_index('day_type')['data'].to_dict())\n",
    "       # .groupby('stop_id')['direction_id','data']\n",
    "       # .apply(lambda x: x.set_index('direction_id')['data'].to_dict())\n",
    "       # .to_frame(orient='index')\n",
    "       # .to_json('test_json6.json',orient='index',indent=4)\n",
    "       )\n",
    "\n",
    "# print(d)\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_direction_id_by_stop_id_by_day_type = df.groupby(['direction_id','stop_id','day_type']).apply(lambda x: x.rename(columns=d).to_dict('records'))\n",
    "\n",
    "\n",
    "# need to get the direction_id to be the key at the top level\n",
    "# need to get the stop_id to be the key at the second level - done\n",
    "# need to get the day_type to be the key at the third level - done\n",
    "# need to get the departure_time to be the value at the fourth level - done\n",
    "\n",
    "\n",
    "\n",
    "d = (df.groupby(['direction_id','stop_id','day_type'])['departure_time','day_type']\n",
    "       .apply(lambda x: x.to_dict('r'))\n",
    "       .reset_index(name='data')\n",
    "       .groupby(['direction_id','stop_id'])['day_type','data']\n",
    "       .apply(lambda x: x.set_index('day_type')['data'].to_dict())\n",
    "       .reset_index(name='departure_times')\n",
    "       # .apply(lambda x: x.set_index('day_type')['data'].to_dict())\n",
    "       # .groupby('stop_id')['direction_id','data']\n",
    "       # .apply(lambda x: x.set_index('direction_id')['data'].to_dict())\n",
    "       # .to_json('test_json6.json',orient='index',indent=4)\n",
    "       )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal `json` v2\n",
    "\n",
    "``` js\n",
    "{\n",
    "    route_id: '',\n",
    "    '0': {\n",
    "        weekday: [\n",
    "            {\n",
    "                stop_sequence: 1,\n",
    "                stop_id: '',\n",
    "                stop_name: '',\n",
    "                coordinate: '',\n",
    "                departure_times: ['', '', '']\n",
    "            },{\n",
    "                stop_sequence: 2,\n",
    "                stop_id: '',\n",
    "                stop_name: '',\n",
    "                coordinate: '',\n",
    "                departure_times: ['', '', '']\n",
    "            },{\n",
    "                stop_sequence: 3,\n",
    "                stop_id: '',\n",
    "                stop_name: '',\n",
    "                coordinate: '',\n",
    "                departure_times: ['', '', '']\n",
    "            }\n",
    "        ],\n",
    "        saturday: [\n",
    "\n",
    "        ],\n",
    "        sunday: [\n",
    "\n",
    "        ]\n",
    "    },\n",
    "    '1': {\n",
    "        \n",
    "    }\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal `json`\n",
    "\n",
    "``` js\n",
    "{\n",
    "  route_id: '',\n",
    "  direction_0:\n",
    "  [\n",
    "    {\n",
    "      stop_id: '',\n",
    "      stop_name: '',\n",
    "      stop_sequence: '',\n",
    "      coordinate: '',\n",
    "      departure_times: \n",
    "      {\n",
    "        weekday:\n",
    "        [\n",
    "          {\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            departure_time: ''\n",
    "          }\n",
    "        ],\n",
    "        saturday:\n",
    "        [\n",
    "          {\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          }\n",
    "        ],\n",
    "        sunday:\n",
    "        [\n",
    "          {\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    },{\n",
    "      stop_id: '',\n",
    "      stop_name: '',\n",
    "      stop_sequence: '',\n",
    "      coordinate: '',\n",
    "      departure_times: \n",
    "      {\n",
    "        weekday:\n",
    "        [\n",
    "          {\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          }\n",
    "        ],\n",
    "        saturday:\n",
    "        [\n",
    "          {\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          }\n",
    "        ],\n",
    "        sunday:\n",
    "        [\n",
    "          {\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          },{\n",
    "            trip_id: '',\n",
    "            departure_time: ''\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  direction_1: \n",
    "  [\n",
    "    \n",
    "  ]\n",
    "}\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c263e3016fdbcc18f23c6f6a65675b5fd898a6e9c804caa5b7f1b343f9ad6c95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
